{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b019bb29",
   "metadata": {},
   "source": [
    "# Unsupervised Clustering using Hidden Markov Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5365fe",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3500257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from hmmlearn import hmm\n",
    "from simpl_eeg import (\n",
    "    connectivity,\n",
    "    eeg_objects,\n",
    "    raw_voltage,\n",
    "    topomap_2d,\n",
    "    topomap_3d_brain,\n",
    "    topomap_3d_head,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a28c248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47659c2f",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e6d0a8",
   "metadata": {},
   "source": [
    "Electroencephalograms (EEG) is an electrophysiological measurement method used to examine the electrical activity of the brain and represent it as location-based channels of waves and frequencies. Essentially, the EEG data from our dataset is recorded from 19 electrodes nodes for 1.5 hours. Therefore, the EEG data is in high dimensionality and could be represented as a multivariate time series data. If we present the data in a tabular format, the number of rows would be the time stamps and the number of columns would be the different electrodes. As we have 1.5 hrs experiment data and each seconds is recorded at 2048 Hz, which means we have 2048 EEG data readings per second, our dataset is large with at least 1 million rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d6afb8",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aec312",
   "metadata": {},
   "source": [
    "EEG data is widely use in diagnosing brain disorders such as epilepsy and brain damage from head injuries, however, with the complexity of data and its dynamic changes over time, it is hard to identify any significant patterns by simply reading the data or visualizing it. The main objective of this strech goal is to find similar patterns from the combination of EEG signals of all 19 electrodes for a given time section from the dataset. In plain English, it is to cluster the brain states for different time periods in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c29c56",
   "metadata": {},
   "source": [
    "## Why Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab3b87c",
   "metadata": {},
   "source": [
    "A Markov model assumes that the future is conditionally independent of the past given the present (Daniel & James, 2020), with the probability shown below:\n",
    "$$P(S_i |S_1...S_{i−1}) = P(S_i|S_{i−1})$$ where $S_i$ is the state at time i.\n",
    "</br>\n",
    "A hidden Markov model (HMM) relates a sequence of observations to a sequence of hidden states that explain the observations (Daniel & James, 2020). For the EEG data, the sequence of observations is the EEG data per time frame and the sequence of hidden states would be the brain states in the dataset. Since the brain activities at time $i$ is less likely to highly correlate to brain activitiees before time $i-1$, the Markov model assumption would be satisfied at this case and therefore we would like to try to apply the hidden Markov model to EEG data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ca136",
   "metadata": {},
   "source": [
    "Since we don't know have labeled data or pre-defined brain states, we would need to use unsupervised HMM for this task. The process of finding the sequence of hidden states given the sequence of observations using HMM is called decoding (Daniel & James, 2020) and the `Viterbi` algorithm is commonly used for decoding. Therefore, for this notebook, I would use the `Viterbi` algorithm in the HMM model for finding the potential brain states. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503c4ff",
   "metadata": {},
   "source": [
    "A hidden Markov model consists of 5 components:\n",
    "- the state space: a set of hidden states\n",
    "- the sequence of observation\n",
    "- the transition probability matrix: the probability transitioning from state $i$ to state $j$\n",
    "- the emission probabilities: conditional probabilities for all observations given a hidden state\n",
    "- the initial probability over states: the probability for the Markov model starts at state $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d626df5",
   "metadata": {},
   "source": [
    "The goal for this task is to explore the set of hidden states (the state space) and the transition probability matrix of the EEG data using hidden Markov model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c05fc",
   "metadata": {},
   "source": [
    "## Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2cc1fe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\Yiki\\Documents\\UBC\\MDS\\Homework\\capstone\\simpl_eeg_capstone\\data\\927\\fixica.fdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-57-99b5e4cf8c83>:1: RuntimeWarning: Data file name in EEG.data (927 fix ica correct.fdt) is incorrect, the file name must have changed on disk, using the correct file name (fixica.fdt).\n",
      "  raw_full = mne.io.read_raw_eeglab(\"../../data/927/fixica.set\")\n"
     ]
    }
   ],
   "source": [
    "raw_full = mne.io.read_raw_eeglab(\"../../data/927/fixica.set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bde7042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_df = raw_full.to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd3ce238",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names=raw_full.ch_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c259da",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc092d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683620aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=entire_df['Fp1'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=entire_df['O2'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf22d6",
   "metadata": {},
   "source": [
    "According to the EDA, the EEG data is following Gaussian distribution, therefore, a GaussianHMM model would be used. However, there are clearly some outliers in the data. My next step is to remove the outliers of the data based on the expertise suggestions from the partner to keep EEG data which falls into (-50, 50) range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba0b9c",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3121a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where all values are zero\n",
    "cleaned_df = entire_df.loc[(entire_df[channel_names] != 0).all(axis=1)]\n",
    "\n",
    "# drop the outliers of dataset (only keep rows where EEG voltage is between -50 to 50)\n",
    "df_no_outliers = cleaned_df.loc[((cleaned_df[channel_names] <=50) & (cleaned_df[channel_names] >= -50)).all(axis=1)]\n",
    "\n",
    "# chunk the data into per second (each second has 2048 readings (rows))\n",
    "df_second = np.split(df_no_outliers, range(2048, len(df_no_outliers), 2048))\n",
    "\n",
    "# for each second, randomly sampled 10 time stamps (the original dataset is too big, wants to sample a smaller dataset for exploration)\n",
    "df_second_resample={}\n",
    "for second in range(len(df_second)):\n",
    "    df_second_resample[second] = df_second[second].sample(10, random_state=2020, axis=0).sort_values(by=\"time\")\n",
    "df_resampled = pd.concat([values for key, values in df_second_resample.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941816e",
   "metadata": {},
   "source": [
    "## Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d657640",
   "metadata": {},
   "source": [
    "Based on the suggestion from the partner, we would like to explore the data in per 5 second interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ab0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_second_df = np.split(df_resampled, range(50, len(df_resampled), 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5723215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since HMM model only takes in (n_sample, n_feature) array, reshape the data into an array where each sample has 50 time stamps (5 seconds data)\n",
    "chunked_list = []\n",
    "for i in range(len(five_second_df)):\n",
    "    chunked_list.append(np.array(five_second_df[i].iloc[:, 1:]).flatten())\n",
    "chunked_array = np.array(chunked_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0651582",
   "metadata": {},
   "source": [
    "As mentioned above, the EEG data follows Gaussian distribution and is continous, we would use the `GaussianHMM` model from `hmmlearn` package. Since we don't know the number of brain states from the model, we would like to start with some random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components is the number of hidden states (number of brain states)\n",
    "chunked_model = hmm.GaussianHMM(n_components=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c68f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_model.fit(chunked_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_result = chunked_model.decode(chunked_array, algorithm=\"viterbi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f275918",
   "metadata": {},
   "source": [
    "### Check model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the metric that hmmlearn package itself used to evaluate the model\n",
    "print(f\"The log probability for this 4-cluster model is {chunked_result[0]:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db23773",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The prior probability for this model is: {chunked_model.startprob_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b91de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The transmission probability matrix of this model is: \\n {chunked_model.transmat_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lables back to the df\n",
    "df_result = five_second_df.copy()\n",
    "for i in range(len(df_result)):\n",
    "    df_result[i]=df_result[i].assign(cluster = chunked_result[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3babac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result=pd.concat([df_result[i] for i in range(len(df_result))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23017441",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d051d",
   "metadata": {},
   "source": [
    "### Visualize output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d1a7e",
   "metadata": {},
   "source": [
    "#### Visualize the average voltage for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = df_result[df_result['cluster'] == 0]\n",
    "cluster_1 = df_result[df_result['cluster'] == 1]\n",
    "cluster_2 = df_result[df_result['cluster'] == 2]\n",
    "cluster_3 = df_result[df_result['cluster'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1453b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "topomap_2d.plot_topomap_2d(raw_full, cluster_0.iloc[:, 1:20].mean().values*1e-6, mark=\"channel_name\",  cmin=-0.5, cmax=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca675a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topomap_2d.plot_topomap_2d(raw_full, cluster_1.iloc[:, 1:20].mean().values*1e-6, mark=\"channel_name\", cmin=-1, cmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab26a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topomap_2d.plot_topomap_2d(raw_full, cluster_2.iloc[:, 1:20].mean().values*1e-6, mark=\"channel_name\",  cmin=-0.3, cmax=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "topomap_2d.plot_topomap_2d(raw_full, cluster_3.iloc[:, 1:20].mean().values*1e-6, mark=\"channel_name\", cmin=-2, cmax=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd4b6e",
   "metadata": {},
   "source": [
    "#### Visualize in an animated way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_array = np.array(df_result.cluster)\n",
    "change_index = np.where(cluster_array[:-1] != cluster_array[1:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a16fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = df_result.iloc[:change_index[0]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_batch = first_batch.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342044f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig, ax = plt.subplots()\n",
    "def animate(frame_number):\n",
    "    fig.clear()\n",
    "#     ax.clear()\n",
    "    fig = topomap_2d.plot_topomap_2d(raw_full, try_batch.iloc[frame_number, 1:20], mark=\"channel_name\")\n",
    "#     plot_static=plt.plot(1,2)\n",
    "    return fig\n",
    "ani = animation.FuncAnimation(\n",
    "        fig,\n",
    "        animate,\n",
    "        frames=range(len(try_batch)),\n",
    "        blit=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21830d4b",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a291a0",
   "metadata": {},
   "source": [
    "Due to the limited time and efforts that we could allocate to this task, there are other potential useful approaches to try for this task but haven't been implemented yet. \n",
    "\n",
    "- Data preprocessing: instead of sampling only 10 time stamps per second, increase the sampling rate so that it could capture more dynmaics from each second to provide a more accurate resutl.\n",
    "\n",
    "- Feature engineering: instead of only using the raw voltage data for model input, include some engineered features that could provide a better representation of the temporal dependencies of the data such as the following:\n",
    "    - apply rolling mean for each 5 second data chunks rather than simply taking the mean of each 5 second data chunks\n",
    "    - use the sliding window approach to slide the per 5 second data \n",
    "\n",
    "\n",
    "- Literature review: read through more literature articles to define a better metric to evaluate the model\n",
    "\n",
    "- Hyperparameter tuning: currently, there isn't a better way to find the optimal `# of cluster` in the model other than finish fitting the model and visualizing the output to check. Use the metric that we could locate from the previous objective to tune the hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5782714",
   "metadata": {},
   "source": [
    "## Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f1333",
   "metadata": {},
   "source": [
    "- Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2020. All\n",
    "rights reserved. Draft of December 30, 2020.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2451fbab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
