{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpl_eeg import eeg_objects\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "\n",
    "import mne\n",
    "from mne.preprocessing import (create_eog_epochs, create_ecg_epochs,\n",
    "                               compute_proj_ecg, compute_proj_eog)\n",
    "import scipy.io\n",
    "import scipy.interpolate\n",
    "from scipy import signal\n",
    "from scipy.cluster.hierarchy import (\n",
    "    average,\n",
    "    complete,\n",
    "    dendrogram,\n",
    "    fcluster,\n",
    "    single,\n",
    "    ward,\n",
    ")\n",
    "\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "import mglearn\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update figure size\n",
    "plt.rcParams['figure.figsize'] = [18, 8]\n",
    "\n",
    "# random state to make results reproducible\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting EEG data\n",
    "\n",
    "We can select the data we want to use with the `simpl_eeg` package. We can either look at individual time steps or time steps averaged over time. We can also specify what time to look at.\n",
    "\n",
    "For more information about making these selections, please see the page on [Creating EEG Objects](https://ubc-mds.github.io/simpl_eeg_capstone/eeg_objects.html) in the `simpl_eeg` documentation. \n",
    "\n",
    "Averaging the data has will reduce the dimensionality of the data, but which method you want to use will depend on what you are trying to achieve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment we want to use data from\n",
    "experiment_number = \"../../data/927\"\n",
    "\n",
    "# set the start second if you want to use a custom time\n",
    "# when the start second is None, the impact times from the experiment will be used. \n",
    "start_second = 500\n",
    "# start_second = None\n",
    "\n",
    "# the number of seconds before the event to use\n",
    "tmin = -5\n",
    "\n",
    "# the number of seconds after the event to use\n",
    "tmax = 5\n",
    "\n",
    "epochs = eeg_objects.Epochs(experiment_number, tmin=tmin, tmax=tmax, start_second=start_second)\n",
    "epoch = epochs.epoch\n",
    "\n",
    "# the lines below are to average every n steps\n",
    "# number of steps to average\n",
    "n = 5\n",
    "\n",
    "averaged_epoch = epochs.average_n_steps(n)\n",
    "\n",
    "print(\"\\nDimensionality difference between raw and averaged:\")\n",
    "print(f\"Raw: {epoch.get_data().shape}\")\n",
    "print(f\"Averaged: {averaged_epoch.get_data().shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which data you want by commenting or uncommenting the lines below:\n",
    "\n",
    "# selected_var = epoch\n",
    "selected_var = averaged_epoch\n",
    "\n",
    "# Convert the data into a dataframe for easy analysis with clustering methods\n",
    "df = selected_var.to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the time column and convert to array\n",
    "df.drop(columns = [\"time\"], inplace=True)\n",
    "X = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "**Clustering** is the task of partitioning the dataset into groups called clusters.\n",
    "\n",
    "The goal of clustering is to discover underlying groups in a given dataset such that:\n",
    "- examples in the same group are as similar as possible;\n",
    "- examples in different groups are as different as possible.          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means using `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**\n",
    "- `X` $\\rightarrow$ a set of data points  \n",
    "- `K` (or $k$ or `n_clusters`) $\\rightarrow$ number of clusters\n",
    "\n",
    "**Output**\n",
    "- `K` clusters (groups) of the data points (Represent each cluster by its cluster center and assign a cluster membership to each data point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of clusters can be selected based on the methods for assessing goodness of fit in the [Quality Assessment](#Quality-Assessment) section below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "n_clusters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the data\n",
    "kmeans_model = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "kmeans_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the clusters\n",
    "predictions = kmeans_model.predict(X)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view cluster centers\n",
    "kmeans_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data frame with the predicted cluster assignment\n",
    "df_predict = df.copy()\n",
    "df_predict[\"Cluster\"] = predictions\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Assessment\n",
    "\n",
    "We used several methods for estimating the best value of the number of clusters (`K`) for the K-means algorithm. A summary of the results of each method is detailed below, with instructions on how to perform them. \n",
    "\n",
    "1)**[The Elbow Method](#The-Elbow-Method)** - determine best value for `K`\n",
    "\n",
    "2)**[The Silhouette Method](#The-Silhouette-Method)** - alternative method to determine best value for `K`\n",
    "\n",
    "3)**[Principal Component Analysis (PCA)](#Principal-Component-Analysis-(PCA))** - visualize clusters in 2D space to see if they are reasonably separated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Elbow Method\n",
    "\n",
    "With the elbow method, you can set a range of values for `K` and visualize the result of the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This method looks at the sum of **intra-cluster distances**, which is also referred to as **inertia**\n",
    "- The inertia decreases as K increases\n",
    "- The intra-cluster distance in our toy example above is given as   \n",
    "\n",
    "$$ \\sum_{P_i \\in C_1}  distance(P_i, C_1)^2 + \\sum_{P_i \\in C_2}  distance(P_i, C_2)^2 + \\sum_{P_i \\in C_3} distance(P_i, C_3)^2$$\n",
    "\n",
    "Where \n",
    "- $C_1, C_2, C_3$ are centroids \n",
    "- $P_i$s are points within that cluster\n",
    "- $distance$ is the usual Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can learn more about the Elbow method [here](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the range of values of K to try\n",
    "k_range = (1, 19)\n",
    "\n",
    "model = KMeans(random_state=random_state)\n",
    "visualizer = KElbowVisualizer(model, k=k_range)\n",
    "visualizer.fit(X)\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "We can see that there is an \"elbow\" starting to form at `K`=4, meaning that this method suggests 4 clusters is the ideal number. It also indicates that the inetria is sdropping at this level. \n",
    "\n",
    "Also, the algortithm computed the average score for all the clusters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Silhouette Method\n",
    "\n",
    "- Not dependent on the notion of cluster centers \n",
    "- Calculated using the **mean intra-cluster distance** ($a$) and the **mean nearest-cluster distance** ($b$) for each sample\n",
    "- the difference between the **the average nearest-cluster distance** ($b$) and **average intra-cluster distance** ($a$) for each sample, normalized by the maximum value\n",
    "\n",
    "$$\\frac{b-a}{max(a,b)}$$\n",
    "\n",
    "- The best value is 1\n",
    "- The worst value is -1 (samples have been assigned to wrong clusters)\n",
    "- Value near 0 means overlapping clusters\n",
    "\n",
    "The overall **Silhouette score** is the average of the Silhouette scores for all samples.\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "-\tThe plots show the Silhouette scores for each sample in that cluster\n",
    "-\tHigher scores indicate well separated clusters\n",
    "-\tThe size represents the size of samples in each cluster\n",
    "-\tThe thickness of each silhouette indicates the cluster size\n",
    "-\tThe shape of each silhouette indicates the \"goodness\" for points in each cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can learn more about the Silhouette Visualizer method [here](https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set k options to try\n",
    "k_options = [3, 4, 5]\n",
    "\n",
    "# store models for later use\n",
    "models = dict()\n",
    "\n",
    "for i in k_options:\n",
    "    model = KMeans(i, random_state=random_state)\n",
    "    visualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\n",
    "    visualizer.fit(X)\n",
    "    visualizer.show();\n",
    "    models[i] = model # store the current model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "From the explained metrics above, we can say that k=3 is the optimal K value given the clusters shape size and the average Silhouette score. It is hard to interpret the plot in these methods. For instatnce the Elbow method indicated K=4 as an optimal while the Silhouette indicated K=3 is optimal in our opinion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "In unsupervised learning techniques such as clustering are based on the notion of distances between points. With increased dimensions, the representation of data becomes more complex.\n",
    "\n",
    "Dimensionality reduction is the task of reducing a dataset in high dimension (our df has many rows) to low dimension while retaining the most \"important\" characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can learn more about PCA [here](https://scikit-learn.org/stable/modules/decomposition.html#pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_clusters(data, labels):\n",
    "    \"\"\"\n",
    "    Carries out dimensionality reduction on the data for visualization\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_comp = pca.fit_transform(data)\n",
    "    pca_df = pd.DataFrame(\n",
    "        data=principal_comp, columns=[\"Principal Component Analysis (PCA)\", \"\"], index=data.index\n",
    "    )\n",
    "    pca_df[\"cluster\"] = labels\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.scatterplot(\n",
    "        x=\"Principal Component Analysis (PCA)\", y=\"\", hue=\"cluster\", data=pca_df, palette=\"tab10\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "for i, model in models.items():\n",
    "    print(f\"PCA for K={i}\")\n",
    "    plot_pca_clusters(df, model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "We can see that the clusters are not very well separated. This indicates that the clusters are not likely quite distinct, however it does not guarantee that the clusters does not represent something useful. In our case, it seems as though the clustering algorithm is picking up on high and low voltage values for creating the clusters. It makes sense that the clusters would look distinct as a result, but the finding is not significant for our goal of identifying brain states given that the voltage values are close to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "- DBSCAN is a density-based clustering algorithm\n",
    "- Intuitively, it's based on the idea that clusters form dense regions in the data and so it works by identifying \"crowded\" regions in the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot functions\n",
    "\n",
    "def plot_X_dbscan(X, model):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    colours = []\n",
    "    if np.any(model.labels_ == -1):\n",
    "        n_clusters = len(set(model.labels_)) - 1 \n",
    "    else: \n",
    "        n_clusters = len(set(model.labels_))\n",
    "    \n",
    "    for i in range(n_clusters + 1):\n",
    "        colours.append(\"#%06X\" % np.random.randint(0, 0xFFFFFF))\n",
    "        \n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], ax=axes[0], markeredgewidth=1.0)\n",
    "\n",
    "    if np.any(model.labels_ == -1):\n",
    "        colours = [\"w\"] + colours\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], model.labels_, c=colours, markers=\"o\", markeredgewidth=1.0, ax=axes[1]);\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def plot_dbscan_with_labels(X, eps=1.0, min_samples = 2, font_size=14):\n",
    "    model = DBSCAN(eps=eps, min_samples=min_samples) \n",
    "    model.fit(X)    \n",
    "    if np.any(model.labels_ == -1):\n",
    "        n_clusters = len(set(model.labels_)) - 1 \n",
    "    else: \n",
    "        n_clusters = len(set(model.labels_))\n",
    "    plt.title('Number of clusters: %d'%(n_clusters))\n",
    "    colours = []\n",
    "    for i in range(n_clusters + 1):\n",
    "        colours.append(\"#%06X\" % np.random.randint(0, 0xFFFFFF))\n",
    "        \n",
    "    #colours = [mglearn.cm3(1), mglearn.cm3(0)]\n",
    "    if np.any(model.labels_ == -1):\n",
    "        colours = [\"w\"] + colours\n",
    "    mglearn.discrete_scatter(\n",
    "        X[:, 0], X[:, 1], model.labels_, c=colours, markers=\"o\", markeredgewidth=1.0\n",
    "    );\n",
    "    plt.legend()\n",
    "    labels = [str(label) for label in list(range(0,len(X)))]\n",
    "    for i, txt in enumerate(labels):\n",
    "        plt.annotate(txt, X[i], xytext=X[i] + 0.2, size = font_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps: determines what it means for points to be \"close\"\n",
    "# min_samples: determines the number of neighboring points we require to consider in order for a point to be part of a cluster\n",
    "\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=2)\n",
    "dbscan.fit(X)\n",
    "interactive(lambda eps=1: plot_X_dbscan(X, dbscan), eps=(1, 50))\n",
    "# plot_dbscan_with_labels(X, dbscan)\n",
    "# interactive(lambda eps=1: plot_dbscan_with_labels(X, eps), eps=(1, 12, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increasing `eps` ($\\uparrow$) (left to right in the plot above) means more points will be included in a cluster.  \n",
    "- Increasing `min_samples` ($\\uparrow$) (top to bottom in the plot above) means points in less dense regions will either be labeled as their own cluster or noise. \n",
    "- In general, it's not trivial to tune these hyperparameters. \n",
    "\n",
    "There are three kinds of points:\n",
    "\n",
    "- Core points are the points that have at least min_samples points in the neighborhood.\n",
    "\n",
    "- Border points are the points with fewer than min_samples points in the neighborhood, but are connected to a core point.\n",
    "\n",
    "- Noise points are the points which do not belong to any cluster. In other words, the points which have less that min_samples point within distance eps of the starting point are noise points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that DBSCAN has identified one cluster only; hence, the crowded region. It also identified all the points as noise! (If you try and change 'min_saples=2' to 'min_samples=1' you'll notice that each data point is in cluster). This is expected given the high dimensionality of our data. Although, DBSCAN did not identify any clusters, it's a very useful tool to use especially if you don't want to specify number of clusters (k). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's evaluate different hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.5, min_samples=2)\n",
    "clusters = dbscan.fit_predict(X)\n",
    "\n",
    "print(\"Cluster assignments:{}\".format(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- noise points: shown in white\n",
    "- core points: bigger\n",
    "- border points: smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_dbscan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we can see after fitting X, that having min_samples = 2 and eps=1.5 will give us better clustering as there won't be any noises to be clustered.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means vs. DBSCAN\n",
    "\n",
    "- In DBSCAN, you do not have to specify the number of clusters! \n",
    "    - Instead, you have to tune `eps` and `min_samples`. \n",
    "- Unlike K-Means, DBSCAN doesn't have to assign all points to clusters. \n",
    "    - The label is -1 if a point is unassigned.\n",
    "- Unlike K-Means, there is no `predict` method. \n",
    "    - DBSCAN only really clusters the points you have, not \"new\" or \"test\" points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploritory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View number of values in each cluster\n",
    "\n",
    "df_predict[\"Cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View general statistics for each cluster\n",
    "\n",
    "for i in range(len(df_predict[\"Cluster\"].unique())):\n",
    "    cluster_df = df_predict[df_predict[\"Cluster\"] == i]\n",
    "    print(f\"\\nCluster {i}\")\n",
    "    print(cluster_df.iloc[:, :5].describe()) # only view first 5 channels\n",
    "    # print(cluster_df.describe()) # uncomment to view all channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all channels at once\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "single_plot_df = df_predict.copy()\n",
    "for i, col in enumerate(single_plot_df.columns):\n",
    "    if col != \"Cluster\":\n",
    "        single_plot_df[col] = single_plot_df[col] + 100*i\n",
    "single_plot_df[\"time\"] = single_plot_df.index\n",
    "single_plot_df = single_plot_df.melt(id_vars=[\"Cluster\", \"time\"])\n",
    "ax = sns.scatterplot(\n",
    "    x=\"time\", y=\"value\", hue=\"Cluster\", data=single_plot_df, palette=\"tab10\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View channels in individual plots\n",
    "\n",
    "plt.figure(figsize=(30, 6))\n",
    "\n",
    "for col in df_predict.columns:\n",
    "    if col != \"Cluster\":\n",
    "        ax = sns.scatterplot(\n",
    "            x=df_predict.index, y=col, hue=\"Cluster\", data=df_predict, palette=\"tab10\"\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "We can clearly see that the K-means algorithm clustered the low/negative values in orange and the high/positve values in green. As for the middle cluster groups (green & blue), we need to invistigate further in order to come up with relationship and clear pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between the channels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the correlation between the channels, we need to use statistics test that looks at the relationship between two continues variables and measure the linear correlation between them. A good test that fits here is The Pearson's Correlation Coefficient. It's a linear correlation coefficient that gives values between -1 and 1 where 1 indicates a strong positive correlation and -1 indicates a strong negative correlation. However, for this dataset we will use the Spearman's Rank-Order Correlation method, which is the nonparametric version of the Pearon's method. You may read more about the Spearman's method and reasoning behind using it [here](https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php#:~:text=The%20Spearman%27s%20rank%2Dorder%20correlation%20is%20the%20nonparametric%20version%20of,association%20between%20two%20ranked%20variables.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the heat correlation heatmap\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "sns.set(font_scale=1)\n",
    "df_spearman = df.corr('spearman')\n",
    "sns.heatmap(df_spearman, annot=True, cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Elbow method to determine the number of clusters for the spearman dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = (3, 10)\n",
    "\n",
    "model = KMeans(random_state=random_state)\n",
    "visualizer = KElbowVisualizer(model, k=k_range)\n",
    "visualizer.fit(df_spearman)\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit k=5 \n",
    "kmeans_model = KMeans(n_clusters=5, random_state=random_state)\n",
    "kmeans_model.fit(df_spearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = kmeans_model.predict(df_spearman)\n",
    "predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Clusters column\n",
    "df_spearman['Clusters'] = predictions2\n",
    "df_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the df based on cluster values and drop the Clusters column\n",
    "df_sort2 = df_spearman.sort_values('Clusters')\n",
    "df_sort3 = df_sort2[list(df_sort2.index)]\n",
    "df_sort3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap for the clustered dataframe\n",
    "sns.set(font_scale=1)\n",
    "ax = sns.heatmap(df_sort3.iloc[:,:-1], annot=True, cmap=plt.cm.Blues)\n",
    "ax.hlines([8, 9, 14,16], color = 'limegreen', *ax.get_xlim(), linewidths=3) # numbers are coordinates for the axis\n",
    "ax.vlines([8, 9, 14,16], color = 'limegreen', *ax.get_ylim(), linewidths=3) # numbers are coordinates for the axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how Fp2 is noisy and suspecious hence it got clustered by itself. The green lines represent the boundaries between clusters. Also, we can see that nodes that are close to each other tend to be more correlated as seen by the darker blue sections along the diagonal of the heatmap and lighter blue near the outsides. This makes sense intuitively, because when a change in voltage occurs it may be picked up by multiple channels.\n",
    "\n",
    "We can also examine the correlation of Fp2 using our topomap_2d function for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpl_eeg import topomap_2d, eeg_objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topomap_2d.plot_topomap_2d(\n",
    "        epoch,\n",
    "        np.array(df_spearman['Fp2']),\n",
    "        mark=\"channel_name\",\n",
    "        cmin=0,\n",
    "        cmax=1,\n",
    "        colormap=\"tab10\",\n",
    "        **{'image_interp':'none'}\n",
    "\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution\n",
    "\n",
    "- Most of the material of this notebook came form DSCI_563 using the following license agreement\n",
    "\n",
    "The following MIT License is applied to the code contained in this repository. The intent is for MDS students to be able to refer back to their course notes and reuse code for future projects. Students: note that the MIT License requires including the copyright/permission notice with the code.\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2021 Varada Kolhatkar, Rodolfo Lourenzutti, Mike Gelbart\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "https://github.ubc.ca/MDS-2020-21/DSCI_563_unsup-learn_students/blob/master/LICENSE.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
