{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3827ae8c",
   "metadata": {},
   "source": [
    "# Hidden Markov Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8daf6e5",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from hmmlearn import hmm\n",
    "from simpl_eeg import (\n",
    "    connectivity,\n",
    "    eeg_objects,\n",
    "    raw_voltage,\n",
    "    topomap_2d,\n",
    "    topomap_3d_brain,\n",
    "    topomap_3d_head,\n",
    ")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f7eda2",
   "metadata": {},
   "source": [
    "## Why Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f461eec",
   "metadata": {},
   "source": [
    "A Markov model assumes that the future is conditionally independent of the past given the present (Daniel & James, 2020), with the probability shown below:\n",
    "\n",
    "\n",
    "$$P(S_i |S_1...S_{i−1}) = P(S_i|S_{i−1})$$ \n",
    "\n",
    "\n",
    "where $S_i$ is the state at time i.\n",
    "\n",
    "A hidden Markov model (HMM) relates a sequence of observations to a sequence of hidden states that explain the observations (Daniel & James, 2020). For the EEG data, the sequence of observations is the EEG data per time frame and the sequence of hidden states would be the brain states in the dataset. Since the brain activities at time $i$ is less likely to highly correlate to brain activities before time $i-1$, the Markov model assumption would be satisfied at this case and therefore we would like to try to apply the hidden Markov model to EEG data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffc614",
   "metadata": {},
   "source": [
    "Since we don't have labeled data or pre-defined brain states, we would need to use unsupervised HMM for this task. The process of finding the sequence of hidden states given the sequence of observations using HMM is called decoding (Daniel & James, 2020) and the `Viterbi` algorithm is commonly used for decoding. Therefore, for this notebook, I would use the `Viterbi` algorithm in the HMM model for finding the potential brain states. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5b1f0",
   "metadata": {},
   "source": [
    "A hidden Markov model consists of 5 components:\n",
    "- the state space: a set of hidden states\n",
    "- the sequence of observation\n",
    "- the transition probability matrix: the probability transitioning from state $i$ to state $j$\n",
    "- the emission probabilities: conditional probabilities for all observations given a hidden state\n",
    "- the initial probability over states: the probability for the Markov model starts at state $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94d15e",
   "metadata": {},
   "source": [
    "The goal for this task is to explore the set of hidden states (the state space) and the transition probability matrix of the EEG data using hidden Markov model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d4678",
   "metadata": {},
   "source": [
    "## Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d21409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_full = mne.io.read_raw_eeglab(\"../../data/927/fixica.set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_df = raw_full.to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names = raw_full.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb6365",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = eeg_objects.Epochs(\"../../data/927\").epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b4da4",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a173d00",
   "metadata": {},
   "source": [
    "According to the EDA in the [intro page](https://ubc-mds.github.io/simpl_eeg_capstone/clustering.html), the EEG data is following Gaussian distribution for each electrode node, therefore, a GaussianHMM model would be used. However, there are clearly some outliers in the data. My next step is to remove the outliers of the data based on the expertise suggestions from the partner to keep EEG data which falls into (-50, 50) range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dff499",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea60b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so make sure everytime the notebook is generating the same outcome\n",
    "np.random.seed(2021)\n",
    "\n",
    "# drop rows where all values are zero\n",
    "cleaned_df = entire_df.loc[(entire_df[channel_names] != 0).all(axis=1)]\n",
    "\n",
    "# drop the outliers of dataset (only keep rows where EEG voltage is between -50 to 50)\n",
    "df_no_outliers = cleaned_df.loc[\n",
    "    ((cleaned_df[channel_names] <= 50) & (cleaned_df[channel_names] >= -50)).all(axis=1)\n",
    "]\n",
    "\n",
    "# chunk the data into per second (each second has 2048 readings (rows))\n",
    "df_second = np.split(df_no_outliers, range(2048, len(df_no_outliers), 2048))\n",
    "\n",
    "\n",
    "# for each second, randomly sampled 10 time stamps (the original dataset is too big, wants to sample a smaller dataset for exploration)\n",
    "time_jump = 50\n",
    "df_second_resample = {}\n",
    "for second in range(len(df_second)):\n",
    "    df_second_resample[second] = (\n",
    "        df_second[second]\n",
    "        .sample(time_jump, random_state=2020, axis=0)\n",
    "        .sort_values(by=\"time\")\n",
    "    )\n",
    "df_resampled = pd.concat([values for key, values in df_second_resample.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af8df6",
   "metadata": {},
   "source": [
    "## Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613498a",
   "metadata": {},
   "source": [
    "Based on the suggestion from the partner, we would like to explore the data in per 5 second interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a492f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_second_df = np.split(\n",
    "    df_resampled, range(time_jump * 5, len(df_resampled), time_jump * 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since HMM model only takes in (n_sample, n_feature) array, reshape the data into an array where each sample has 5 seconds data\n",
    "chunked_list = []\n",
    "for i in range(len(five_second_df)):\n",
    "    chunked_list.append(np.array(five_second_df[i].iloc[:, 1:]).flatten())\n",
    "chunked_array = np.array(chunked_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa65c04",
   "metadata": {},
   "source": [
    "As mentioned above, the EEG data follows Gaussian distribution and is continous, we would use the `GaussianHMM` model from `hmmlearn` package. Since we don't know the number of brain states from the model, we would like to start with some random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so make sure everytime the notebook is generating the same outcome\n",
    "np.random.seed(2021)\n",
    "\n",
    "# n_components is the number of hidden states (number of brain states)\n",
    "chunked_model = hmm.GaussianHMM(n_components=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_model.fit(chunked_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a360d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so make sure everytime the notebook is generating the same outcome\n",
    "np.random.seed(2021)\n",
    "\n",
    "chunked_result = chunked_model.decode(chunked_array, algorithm=\"viterbi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e510a",
   "metadata": {},
   "source": [
    "### Check model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e252e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the metric that hmmlearn package itself used to evaluate the model\n",
    "print(\n",
    "    f\"The log probability for this {chunked_model.n_components}-cluster model is {chunked_result[0]:0.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The starting probability for this model is: {chunked_model.startprob_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eb52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The transmission probability matrix of this model is: \\n\")\n",
    "pd.DataFrame(chunked_model.transmat_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bdeea5",
   "metadata": {},
   "source": [
    "#### Interpretation of the output\n",
    "\n",
    "- Based on the starting probability, the dataset starts with the a brain state with probability equals to 1.\n",
    "- It is hard to tell whether there are any dominating brain states from the transmission probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33528dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lables back to the df\n",
    "df_result = five_second_df.copy()\n",
    "for i in range(len(df_result)):\n",
    "    df_result[i] = df_result[i].assign(cluster=chunked_result[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.concat([df_result[i] for i in range(len(df_result))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe1bf5",
   "metadata": {},
   "source": [
    "### Output visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378192e",
   "metadata": {},
   "source": [
    "#### Reason why I use visualization methods to assess the output\n",
    "\n",
    "- The log probability that HMM model itself provides doesn't provide contents to evalute the model performance.\n",
    "- It is hard to evaluate the model performance by only looking at the raw voltage values.\n",
    "- Visualizing the EEG data seems to be the most intuitive way to check the output.\n",
    "- There is not a good way to determine the optimal `n_components` (which is the number of brain states in the data) of the HMM model. The best way is to check the output for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681949dd",
   "metadata": {},
   "source": [
    "#### Visualize the average voltage for each cluster in the resampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78985796",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict = {}\n",
    "for i in range(chunked_model.n_components):\n",
    "    cluster_dict[\"cluster_\" + str(i)] = df_result[df_result[\"cluster\"] == i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_key, cluster in cluster_dict.items():\n",
    "    cluster_key = topomap_2d.plot_topomap_2d(\n",
    "        epoch,\n",
    "        cluster.iloc[:, 1:20].mean().values * 1e-6,\n",
    "        mark=\"channel_name\",\n",
    "        cmin=-0.8,\n",
    "        cmax=0.8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5b335",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "- Each topographic heatmap shows the **average raw voltage** for each electrode node for a specific cluster.\n",
    "- Although each clusters does show different patterns, it is hard to define the brain states by just looking at it.\n",
    "- Although there are different patterns in each clusters, the differences are not significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38299da6",
   "metadata": {},
   "source": [
    "#### Label the original dataset and visualize the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e2b48c",
   "metadata": {},
   "source": [
    "Since we were only drawing a random sample of 50 time stamps for each second for fitting the model, we wanted to add the cluster labels back to the entire dataset to check the outcome to see whether they make a good representation of each second. If the cluster outcomes look very different from the resampled data outcomes, we might want to increase the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lables back to the entire_df\n",
    "entire_df_result = np.split(df_no_outliers, range(10240, len(df_no_outliers), 10240))\n",
    "for i in range(len(entire_df_result)):\n",
    "    entire_df_result[i] = entire_df_result[i].assign(cluster=chunked_result[1][i])\n",
    "\n",
    "entire_df_result = pd.concat(\n",
    "    [entire_df_result[i] for i in range(len(entire_df_result))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f227d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the clusters for the entire dataset\n",
    "entire_cluster_dict = {}\n",
    "for i in range(chunked_model.n_components):\n",
    "    entire_cluster_dict[\"cluster_\" + str(i)] = entire_df_result[\n",
    "        entire_df_result[\"cluster\"] == i\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe0aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_key, cluster in entire_cluster_dict.items():\n",
    "    cluster_key = topomap_2d.plot_topomap_2d(\n",
    "        epoch,\n",
    "        cluster.iloc[:, 1:20].mean().values * 1e-6,\n",
    "        mark=\"channel_name\",\n",
    "        cmin=-0.5,\n",
    "        cmax=0.5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c1f468",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "- Although the color patterns don't exactly match with the resampled data, the patterns seem to be consistent. 50 randomly sampled time stamps data seems to serve as a good representation for the entire second. But I would still recommend trying to increase the sampling size if it is computationally feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c048154",
   "metadata": {},
   "source": [
    "#### Visualize the raw voltage plot for a couple seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce60664",
   "metadata": {},
   "source": [
    "Since just assessing the outcome using the topographic heatmap doesn't provide a lot of information, I would like to generate a raw voltage plot to visualize the output. However, since the `raw_voltage` function of the `simpl_eeg` package doesn't accept data frame and don't have access to overwrite the epoch data to include the cluster label, I will use the line plot from `plotly` package. Since the entire dataset is too large, I am only looking at a screenshot of **a five second time period** for each clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96947514",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [\n",
    "    \"red\",\n",
    "    \"yellow\",\n",
    "    \"green\",\n",
    "    \"blue\",\n",
    "    \"black\",\n",
    "    \"brown\",\n",
    "    \"grey\",\n",
    "    \"chocolate\",\n",
    "    \"crimson\",\n",
    "    \"coral\",\n",
    "    \"darkgoldenrod\",\n",
    "    \"orange\",\n",
    "    \"purple\",\n",
    "    \"burlywood\",\n",
    "    \"cornflowerblue\",\n",
    "    \"darkblue\",\n",
    "    \"darkviolet\",\n",
    "    \"darkorange\",\n",
    "    \"darkgray\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de393340",
   "metadata": {},
   "source": [
    "```python\n",
    "color_count = 0\n",
    "for cluster, df in entire_cluster_dict.items():\n",
    "    sliced_df = df.iloc[:10240]\n",
    "    single_plot_df = sliced_df.copy()\n",
    "    for i, col in enumerate(single_plot_df.columns):\n",
    "        if col != \"cluster\":\n",
    "            single_plot_df[col] = single_plot_df[col] + 100 * i\n",
    "    single_plot_df[\"time\"] = single_plot_df.index\n",
    "    single_plot_df = single_plot_df.melt(id_vars=[\"cluster\", \"time\"])\n",
    "    line_fig = go.Figure()\n",
    "    line_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=single_plot_df[\"time\"],\n",
    "            y=single_plot_df[\"value\"],\n",
    "            mode=\"lines\",\n",
    "            name=cluster,\n",
    "            line=dict(color=color_list[color_count]),\n",
    "            hoverinfo=\"skip\",\n",
    "        )\n",
    "    )\n",
    "    line_fig.update_layout(title=cluster)\n",
    "    line_fig.update_xaxes(showticklabels=False)\n",
    "    line_fig.update_yaxes(title=\"Channel\", showticklabels=False)\n",
    "    line_fig.show()\n",
    "    color_count += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6234d2",
   "metadata": {},
   "source": [
    "![](instruction_imgs/clustering_img/cluster_0.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_1.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_2.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_3.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_4.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_5.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_6.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_7.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_8.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_9.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_10.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_11.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_12.jpg)\n",
    "![](instruction_imgs/clustering_img/cluster_13.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79382067",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "- **The time interval for each of the clusters above is 5 seconds.**\n",
    "- There are clearly differences in each of clusters when looking at the raw voltage plot. However, it is hard to interpret whether these patterns are providing any specific information without EEG background knowledge.\n",
    "- Within the same cluster, the brain activities show similar patterns across different electrode nodes (not exactly the same but similar). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bc097",
   "metadata": {},
   "source": [
    "## Comments about the Hidden Markov Model\n",
    "\n",
    "- It is really hard to interpret the model and to tune the hyperparameter (`n_components`) for the model.\n",
    "- Model performance is hard to assess and requires a lot of background knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b31dc7",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9787b",
   "metadata": {},
   "source": [
    "Due to the limited time and efforts that we could allocate to this task, there are other potential useful approaches to try for this task but haven't been implemented yet. \n",
    "\n",
    "- Data preprocessing: \n",
    "    - instead of sampling only 50 time stamps per second, increase the sampling rate so that it could capture more dynmaics from each second to provide a more accurate result.\n",
    "    - instead of looking at the entire dataset, subset the dataset into epoches and then use epoched data to fit the model.\n",
    "\n",
    "\n",
    "- Feature engineering: instead of only using the raw voltage data for model input, include some engineered features that could provide a better representation of the temporal dependencies of the data such as the following:\n",
    "    - apply rolling mean for each 5 second data chunks rather than simply taking the mean of each 5 second data chunks\n",
    "    - use the sliding window approach to slide the per 5 second data \n",
    "\n",
    "\n",
    "- Literature review: read through more literature articles to define a better metric to evaluate the model\n",
    "\n",
    "- Hyperparameter tuning: currently, there isn't a better way to find the optimal `# of cluster` in the model other than finish fitting the model and visualizing the output to check. Use the metric that we could locate from the previous objective to tune the hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aee5da",
   "metadata": {},
   "source": [
    "## Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d976a27",
   "metadata": {},
   "source": [
    "- Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2020. All\n",
    "rights reserved. Draft of December 30, 2020.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d0cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
